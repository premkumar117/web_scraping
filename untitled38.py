# -*- coding: utf-8 -*-
"""Untitled38.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mD2ORqtfIh45dQQw7I9KLa2v1xyoRKsC
"""

import requests
from bs4 import BeautifulSoup

def scrape_amazon_products(url, num_pages):
    for page in range(1, num_pages + 1):
        page_url = f"{url}&page={page}"
        response = requests.get(page_url)

        if response.status_code == 200:
            soup = BeautifulSoup(response.content, "html.parser")
            product_containers = soup.find_all("div", class_="s-result-item")

            for product_container in product_containers:
                product_link = product_container.find("a", class_="a-link-normal")
                if product_link:
                    product_url = "https://www.amazon.in" + product_link["href"]
                    product_name = product_link.find("span", class_="a-text-normal")
                    if product_name:
                        product_name = product_name.text.strip()

                        product_price = product_container.find("span", class_="a-offscreen")
                        if product_price:
                            product_price = product_price.text.strip()

                            rating = product_container.find("span", class_="a-icon-alt")
                            if rating:
                                rating = rating.text.strip()

                                num_reviews = product_container.find("span", {"class": "a-size-base", "dir": "auto"})
                                if num_reviews:
                                    num_reviews = num_reviews.text.strip()

                                    print("Product URL:", product_url)
                                    print("Product Name:", product_name)
                                    print("Product Price:", product_price)
                                    print("Rating:", rating)
                                    print("Number of Reviews:", num_reviews)
                                    print("-" * 50)
                                else:
                                    print("Number of Reviews not found.")
                            else:
                                print("Rating not found.")
                        else:
                            print("Product Price not found.")
                    else:
                        print("Product Name not found.")
                else:
                    print("Product Link not found.")
        else:
            print(f"Failed to fetch page {page}")

if __name__ == "__main__":
    base_url = "https://www.amazon.in/s?k=bags&crid=2M096C61O4MLT&qid=1653308124&sprefix=ba%2Caps%2C283&ref=sr_pg_1"
    num_pages_to_scrape = 20
    scrape_amazon_products(base_url, num_pages_to_scrape)

import requests
from bs4 import BeautifulSoup
import csv

def scrape_product_details(product_url):
    response = requests.get(product_url)

    if response.status_code == 200:
        soup = BeautifulSoup(response.content, "html.parser")

        description = soup.find("meta", attrs={"name": "description"})["content"]
        asin = soup.find("th", text="ASIN").find_next_sibling("td").text
        product_description = soup.find("div", id="productDescription").get_text(separator="\n")
        manufacturer = soup.find("a", id="bylineInfo").get_text(strip=True)

        return {
            "Description": description,
            "ASIN": asin,
            "Product Description": product_description,
            "Manufacturer": manufacturer
        }
    else:
        print(f"Failed to fetch product details for {product_url}")
        return None

if __name__ == "__main__":
    base_url = "https://www.amazon.in/s?k=bags&crid=2M096C61O4MLT&qid=1653308124&sprefix=ba%2Caps%2C283&ref=sr_pg_1"
    num_pages_to_scrape = 2
    product_urls = []

    for page in range(1, num_pages_to_scrape + 1):
        page_url = f"{base_url}&page={page}"
        response = requests.get(page_url)

        if response.status_code == 200:
            soup = BeautifulSoup(response.content, "html.parser")
            product_containers = soup.find_all("div", class_="s-result-item")

            for product_container in product_containers:
                product_link = product_container.find("a", class_="a-link-normal")
                if product_link:
                    product_url = "https://www.amazon.in" + product_link["href"]
                    product_urls.append(product_url)

    scraped_data = []

    for product_url in product_urls:
        product_details = scrape_product_details(product_url)
        if product_details:
            scraped_data.append(product_details)

    # Export the data to a CSV file
    csv_filename = "scraped_data.csv"
    with open(csv_filename, "w", newline="", encoding="utf-8") as csv_file:
        csv_writer = csv.DictWriter(csv_file, fieldnames=["Description", "ASIN", "Product Description", "Manufacturer"])
        csv_writer.writeheader()
        csv_writer.writerows(scraped_data)

    print(f"Scraped data has been exported to {csv_filename}")